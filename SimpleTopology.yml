name: frr-spine-leaf-hosts-ansible

# 1) Sección mgmt a nivel raíz para personalizar la red de gestión
mgmt:
  network: mgmt-net
  ipv4-subnet: 172.20.20.0/24

# 2) Topología: nodos y enlaces de dataplane
topology:
  nodes:
    # 2 spines
    spine1:
      kind: linux
      image: frr
      mgmt-ipv4: 172.20.20.3
      cap-add: ["NET_ADMIN", "NET_RAW"]
      # Arranca FRR y luego permanece vivo
      cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'"
    spine2:
      kind: linux
      image: frr
      mgmt-ipv4: 172.20.20.2
      cap-add: ["NET_ADMIN", "NET_RAW"]
      # Arranca FRR y luego permanece vivo
      cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'"

    # 4 leafs
    leaf1:
      kind: linux
      image: frr 
      mgmt-ipv4: 172.20.20.11
      cap-add: ["NET_ADMIN", "NET_RAW"]
      # Arranca FRR y luego permanece vivo
      cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'"
    leaf2:
      kind: linux
      image: frr 
      mgmt-ipv4: 172.20.20.12
      cap-add: ["NET_ADMIN", "NET_RAW"]
      # Arranca FRR y luego permanece vivo
      cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'"
    leaf3:
      kind: linux
      image: frr 
      mgmt-ipv4: 172.20.20.13
      cap-add: ["NET_ADMIN", "NET_RAW"]
      # Arranca FRR y luego permanece vivo
      cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'"
    leaf4:
      kind: linux 
      image: frr 
      mgmt-ipv4: 172.20.20.14
      cap-add: ["NET_ADMIN", "NET_RAW"]
      # Arranca FRR y luego permanece vivo
      cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'"

    # 12 hosts (3 por leaf)
    h1:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.21, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h2:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.22, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h3:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.23, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h4:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.24, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h5:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.25, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h6:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.26, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h7:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.27, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h8:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.28, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h9:  { kind: linux, image: frr, mgmt-ipv4: 172.20.20.29, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h10: { kind: linux, image: frr, mgmt-ipv4: 172.20.20.30, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h11: { kind: linux, image: frr, mgmt-ipv4: 172.20.20.31, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }
    h12: { kind: linux, image: frr, mgmt-ipv4: 172.20.20.32, cap-add: ["NET_ADMIN", "NET_RAW"], cmd: "sh -c '/usr/lib/frr/frrinit.sh start && tail -f /dev/null'" }

    # Nodo Ansible, con claves SSH montadas en 'binds' (no 'volumes') :contentReference[oaicite:2]{index=2}
    ansible:
      kind: linux
      image: ansible-station
      mgmt-ipv4: 172.20.20.10
      cap-add: ["NET_ADMIN", "NET_RAW"]
      binds:
        - /var/run/docker.sock:/var/run/docker.sock
        - /proc:/host_proc:ro
        - ./inventory:/home/ansible/inventory:ro
        - ./playbooks:/home/ansible/playbooks:ro
      cmd: "sh -c 'tail -f /dev/null'"

  links:
    # Cada spine ↔ cada leaf: 2 enlaces paralelos
    - endpoints: ["spine1:eth1", "leaf1:eth1"]
    - endpoints: ["spine1:eth2", "leaf1:eth2"]
    - endpoints: ["spine2:eth1", "leaf1:eth3"]
    - endpoints: ["spine2:eth2", "leaf1:eth4"]

    - endpoints: ["spine1:eth3", "leaf2:eth1"]
    - endpoints: ["spine1:eth4", "leaf2:eth2"]
    - endpoints: ["spine2:eth3", "leaf2:eth3"]
    - endpoints: ["spine2:eth4", "leaf2:eth4"]

    - endpoints: ["spine1:eth5", "leaf3:eth1"]
    - endpoints: ["spine1:eth6", "leaf3:eth2"]
    - endpoints: ["spine2:eth5", "leaf3:eth3"]
    - endpoints: ["spine2:eth6", "leaf3:eth4"]

    - endpoints: ["spine1:eth7", "leaf4:eth1"]
    - endpoints: ["spine1:eth8", "leaf4:eth2"]
    - endpoints: ["spine2:eth7", "leaf4:eth3"]
    - endpoints: ["spine2:eth8", "leaf4:eth4"]

    # Hosts → su leaf (1 enlace)
    - endpoints: ["h1:eth1",  "leaf1:eth5"]
    - endpoints: ["h2:eth1",  "leaf1:eth6"]
    - endpoints: ["h3:eth1",  "leaf1:eth7"]

    - endpoints: ["h4:eth1",  "leaf2:eth5"]
    - endpoints: ["h5:eth1",  "leaf2:eth6"]
    - endpoints: ["h6:eth1",  "leaf2:eth7"]

    - endpoints: ["h7:eth1",  "leaf3:eth5"]
    - endpoints: ["h8:eth1",  "leaf3:eth6"]
    - endpoints: ["h9:eth1",  "leaf3:eth7"]

    - endpoints: ["h10:eth1", "leaf4:eth5"]
    - endpoints: ["h11:eth1", "leaf4:eth6"]
    - endpoints: ["h12:eth1", "leaf4:eth7"]
